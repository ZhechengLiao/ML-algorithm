{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/H09ifXwxgT44fu1Ca63d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZhechengLiao/ML-algorithm/blob/master/LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Logistic from Scratch"
      ],
      "metadata": {
        "id": "17E4PgMZqktP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 406,
      "metadata": {
        "id": "p7cEvDehoGIF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import inv\n",
        "from sklearn import datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris = datasets.load_iris()\n",
        "x_train = iris.data[:-50, :2]  # we only take the first two features.\n",
        "y_train = iris.target[:-50]\n",
        "x_test = iris.data[:50, :2]\n",
        "y_test = iris.target[:50]"
      ],
      "metadata": {
        "id": "BKNRxTqLoPOE"
      },
      "execution_count": 407,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(w, x):\n",
        "  z = w.dot(x.T)\n",
        "  return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def loss(y, y_pred):\n",
        "  return -np.mean(y*np.log(y_pred) + (1-y)*np.log(1-y_pred))\n",
        "\n",
        "def gradient(x, y, y_pred):\n",
        "  return 1/x.shape[0] * np.dot(x.T, (y_pred-y))\n",
        "\n",
        "def hessian(x):\n",
        "  return 1/x.shape[0] * np.sum(np.dot(sigmoid(w, x)*(1-sigmoid(w, x)), x.dot(x.T)))\n",
        "\n",
        "def newton(x, y, y_pred):\n",
        "  # f'(x) / f''(x) => grad / H\n",
        "  return gradient(x, y, y_pred) / hessian(x)"
      ],
      "metadata": {
        "id": "xYZTt1hToXD2"
      },
      "execution_count": 408,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = np.zeros(2)\n",
        "lr = 0.1\n",
        "epoch_num = 100\n",
        "loss(y_train, sigmoid(w, x_train))\n",
        "w.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07B4_WueZfem",
        "outputId": "ddece0d1-4ba9-40c6-ab6a-8951a067b008"
      },
      "execution_count": 410,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2,)"
            ]
          },
          "metadata": {},
          "execution_count": 410
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epoch_num):\n",
        "  grad = gradient(x_train, y_train, sigmoid(w, x_train))\n",
        "  w -= lr*grad\n",
        "  l = loss(y_train, sigmoid(w, x_train))\n",
        "  print(f'epoch: {epoch + 1}, loss: {l}, weight: {w}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAu-O1fJqkDl",
        "outputId": "245dac9a-67c8-45bc-adcd-a4cae357d500"
      },
      "execution_count": 389,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1, loss: 0.685802727822415, weight: [ 0.02325 -0.01645]\n",
            "epoch: 2, loss: 0.6793111071790089, weight: [ 0.03581836 -0.03867244]\n",
            "epoch: 3, loss: 0.6729241157600723, weight: [ 0.04824053 -0.06074012]\n",
            "epoch: 4, loss: 0.6666400122825773, weight: [ 0.06056313 -0.0826288 ]\n",
            "epoch: 5, loss: 0.6604570812158791, weight: [ 0.07278727 -0.10433989]\n",
            "epoch: 6, loss: 0.654373620031501, weight: [ 0.08491378 -0.12587496]\n",
            "epoch: 7, loss: 0.6483879402125643, weight: [ 0.09694353 -0.14723559]\n",
            "epoch: 8, loss: 0.6424983681919181, weight: [ 0.10887738 -0.16842337]\n",
            "epoch: 9, loss: 0.6367032462219535, weight: [ 0.12071619 -0.18943992]\n",
            "epoch: 10, loss: 0.6310009331777429, weight: [ 0.13246084 -0.21028683]\n",
            "epoch: 11, loss: 0.6253898052952759, weight: [ 0.14411221 -0.23096575]\n",
            "epoch: 12, loss: 0.6198682568466861, weight: [ 0.15567119 -0.25147828]\n",
            "epoch: 13, loss: 0.6144347007544657, weight: [ 0.16713866 -0.27182607]\n",
            "epoch: 14, loss: 0.6090875691467524, weight: [ 0.17851551 -0.29201075]\n",
            "epoch: 15, loss: 0.6038253138558357, weight: [ 0.18980264 -0.31203395]\n",
            "epoch: 16, loss: 0.5986464068620907, weight: [ 0.20100093 -0.33189732]\n",
            "epoch: 17, loss: 0.5935493406855736, weight: [ 0.21211127 -0.35160249]\n",
            "epoch: 18, loss: 0.5885326287275494, weight: [ 0.22313457 -0.3711511 ]\n",
            "epoch: 19, loss: 0.5835948055642187, weight: [ 0.23407171 -0.39054478]\n",
            "epoch: 20, loss: 0.5787344271949257, weight: [ 0.24492358 -0.40978517]\n",
            "epoch: 21, loss: 0.5739500712471031, weight: [ 0.25569107 -0.42887389]\n",
            "epoch: 22, loss: 0.569240337140199, weight: [ 0.26637507 -0.44781256]\n",
            "epoch: 23, loss: 0.5646038462107996, weight: [ 0.27697646 -0.46660281]\n",
            "epoch: 24, loss: 0.5600392418011209, weight: [ 0.28749611 -0.48524623]\n",
            "epoch: 25, loss: 0.5555451893130021, weight: [ 0.29793492 -0.50374443]\n",
            "epoch: 26, loss: 0.5511203762294887, weight: [ 0.30829373 -0.52209901]\n",
            "epoch: 27, loss: 0.546763512106029, weight: [ 0.31857344 -0.54031154]\n",
            "epoch: 28, loss: 0.5424733285332605, weight: [ 0.32877489 -0.55838361]\n",
            "epoch: 29, loss: 0.5382485790732947, weight: [ 0.33889894 -0.57631678]\n",
            "epoch: 30, loss: 0.5340880391713471, weight: [ 0.34894645 -0.59411259]\n",
            "epoch: 31, loss: 0.5299905060444933, weight: [ 0.35891826 -0.61177261]\n",
            "epoch: 32, loss: 0.5259547985492634, weight: [ 0.36881522 -0.62929835]\n",
            "epoch: 33, loss: 0.521979757029718, weight: [ 0.37863815 -0.64669133]\n",
            "epoch: 34, loss: 0.5180642431475817, weight: [ 0.38838789 -0.66395308]\n",
            "epoch: 35, loss: 0.5142071396959392, weight: [ 0.39806525 -0.68108506]\n",
            "epoch: 36, loss: 0.5104073503979285, weight: [ 0.40767105 -0.69808878]\n",
            "epoch: 37, loss: 0.5066637996918006, weight: [ 0.41720609 -0.7149657 ]\n",
            "epoch: 38, loss: 0.5029754325036445, weight: [ 0.42667117 -0.73171726]\n",
            "epoch: 39, loss: 0.49934121400900877, weight: [ 0.43606709 -0.74834492]\n",
            "epoch: 40, loss: 0.49576012938459124, weight: [ 0.44539463 -0.76485009]\n",
            "epoch: 41, loss: 0.49223118355109513, weight: [ 0.45465457 -0.7812342 ]\n",
            "epoch: 42, loss: 0.48875340090829816, weight: [ 0.46384768 -0.79749863]\n",
            "epoch: 43, loss: 0.4853258250633118, weight: [ 0.47297471 -0.81364477]\n",
            "epoch: 44, loss: 0.4819475185529552, weight: [ 0.48203642 -0.82967399]\n",
            "epoch: 45, loss: 0.4786175625611088, weight: [ 0.49103355 -0.84558763]\n",
            "epoch: 46, loss: 0.4753350566318588, weight: [ 0.49996685 -0.86138705]\n",
            "epoch: 47, loss: 0.4720991183791929, weight: [ 0.50883704 -0.87707355]\n",
            "epoch: 48, loss: 0.4689088831939526, weight: [ 0.51764484 -0.89264845]\n",
            "epoch: 49, loss: 0.465763503948707, weight: [ 0.52639097 -0.90811305]\n",
            "epoch: 50, loss: 0.4626621507011568, weight: [ 0.53507613 -0.92346861]\n",
            "epoch: 51, loss: 0.4596040103966431, weight: [ 0.54370101 -0.93871641]\n",
            "epoch: 52, loss: 0.4565882865702868, weight: [ 0.55226632 -0.95385768]\n",
            "epoch: 53, loss: 0.4536141990492479, weight: [ 0.56077271 -0.96889366]\n",
            "epoch: 54, loss: 0.4506809836555526, weight: [ 0.56922088 -0.98382558]\n",
            "epoch: 55, loss: 0.4477878919099074, weight: [ 0.57761148 -0.99865462]\n",
            "epoch: 56, loss: 0.44493419073687446, weight: [ 0.58594517 -1.01338198]\n",
            "epoch: 57, loss: 0.44211916217176145, weight: [ 0.59422259 -1.02800883]\n",
            "epoch: 58, loss: 0.43934210306954, weight: [ 0.60244439 -1.04253632]\n",
            "epoch: 59, loss: 0.4366023248160843, weight: [ 0.6106112 -1.0569656]\n",
            "epoch: 60, loss: 0.43389915304199095, weight: [ 0.61872364 -1.07129781]\n",
            "epoch: 61, loss: 0.43123192733921484, weight: [ 0.62678233 -1.08553404]\n",
            "epoch: 62, loss: 0.42860000098073864, weight: [ 0.63478787 -1.0996754 ]\n",
            "epoch: 63, loss: 0.4260027406434622, weight: [ 0.64274087 -1.11372297]\n",
            "epoch: 64, loss: 0.4234395261344818, weight: [ 0.65064192 -1.12767783]\n",
            "epoch: 65, loss: 0.42090975012091314, weight: [ 0.6584916  -1.14154102]\n",
            "epoch: 66, loss: 0.41841281786338436, weight: [ 0.66629049 -1.1553136 ]\n",
            "epoch: 67, loss: 0.4159481469533195, weight: [ 0.67403917 -1.16899659]\n",
            "epoch: 68, loss: 0.41351516705410596, weight: [ 0.68173819 -1.182591  ]\n",
            "epoch: 69, loss: 0.4111133196462349, weight: [ 0.6893881  -1.19609784]\n",
            "epoch: 70, loss: 0.40874205777648176, weight: [ 0.69698946 -1.20951809]\n",
            "epoch: 71, loss: 0.4064008458111846, weight: [ 0.7045428  -1.22285273]\n",
            "epoch: 72, loss: 0.40408915919367117, weight: [ 0.71204867 -1.23610271]\n",
            "epoch: 73, loss: 0.4018064842058613, weight: [ 0.71950758 -1.24926899]\n",
            "epoch: 74, loss: 0.399552317734078, weight: [ 0.72692005 -1.26235249]\n",
            "epoch: 75, loss: 0.3973261670390783, weight: [ 0.73428659 -1.27535415]\n",
            "epoch: 76, loss: 0.395127549530312, weight: [ 0.74160772 -1.28827487]\n",
            "epoch: 77, loss: 0.3929559925444095, weight: [ 0.74888392 -1.30111555]\n",
            "epoch: 78, loss: 0.39081103312788956, weight: [ 0.75611569 -1.31387707]\n",
            "epoch: 79, loss: 0.38869221782407287, weight: [ 0.76330351 -1.3265603 ]\n",
            "epoch: 80, loss: 0.3865991024641827, weight: [ 0.77044787 -1.33916611]\n",
            "epoch: 81, loss: 0.384531251962605, weight: [ 0.77754923 -1.35169534]\n",
            "epoch: 82, loss: 0.3824882401162811, weight: [ 0.78460806 -1.36414884]\n",
            "epoch: 83, loss: 0.3804696494081948, weight: [ 0.79162482 -1.37652742]\n",
            "epoch: 84, loss: 0.37847507081491616, weight: [ 0.79859996 -1.38883191]\n",
            "epoch: 85, loss: 0.37650410361816106, weight: [ 0.80553393 -1.40106309]\n",
            "epoch: 86, loss: 0.3745563552203196, weight: [ 0.81242716 -1.41322177]\n",
            "epoch: 87, loss: 0.37263144096390555, weight: [ 0.8192801  -1.42530873]\n",
            "epoch: 88, loss: 0.37072898395487563, weight: [ 0.82609317 -1.43732474]\n",
            "epoch: 89, loss: 0.36884861488976717, weight: [ 0.83286679 -1.44927055]\n",
            "epoch: 90, loss: 0.36698997188659666, weight: [ 0.83960138 -1.46114691]\n",
            "epoch: 91, loss: 0.36515270031946556, weight: [ 0.84629735 -1.47295457]\n",
            "epoch: 92, loss: 0.3633364526568129, weight: [ 0.85295511 -1.48469425]\n",
            "epoch: 93, loss: 0.3615408883032581, weight: [ 0.85957506 -1.49636667]\n",
            "epoch: 94, loss: 0.3597656734449714, weight: [ 0.86615759 -1.50797254]\n",
            "epoch: 95, loss: 0.3580104808985139, weight: [ 0.8727031  -1.51951256]\n",
            "epoch: 96, loss: 0.356274989963083, weight: [ 0.87921196 -1.53098741]\n",
            "epoch: 97, loss: 0.3545588862761037, weight: [ 0.88568455 -1.54239778]\n",
            "epoch: 98, loss: 0.35286186167210304, weight: [ 0.89212125 -1.55374433]\n",
            "epoch: 99, loss: 0.35118361404480447, weight: [ 0.89852243 -1.56502773]\n",
            "epoch: 100, loss: 0.34952384721238167, weight: [ 0.90488845 -1.57624864]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Newton Method"
      ],
      "metadata": {
        "id": "rSi_pgLTtUDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epoch_num):\n",
        "  w -= newton(x_train, y_train, sigmoid(w, x_train))\n",
        "  l = loss(y_train, sigmoid(w, x_train))\n",
        "  print(f'epoch:{epoch}, loss: {l}, weight: {w}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcdot2uAtP5J",
        "outputId": "17c6a66a-c3e6-4aaa-f7ac-9b71f867fe50"
      },
      "execution_count": 390,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0, loss: 0.34950269048499427, weight: [ 0.90496983 -1.57639207]\n",
            "epoch:1, loss: 0.34948153594249876, weight: [ 0.9050512  -1.57653549]\n",
            "epoch:2, loss: 0.34946038358458614, weight: [ 0.90513257 -1.57667891]\n",
            "epoch:3, loss: 0.3494392334109473, weight: [ 0.90521394 -1.57682233]\n",
            "epoch:4, loss: 0.34941808542127356, weight: [ 0.90529531 -1.57696574]\n",
            "epoch:5, loss: 0.3493969396152561, weight: [ 0.90537667 -1.57710914]\n",
            "epoch:6, loss: 0.34937579599258606, weight: [ 0.90545803 -1.57725254]\n",
            "epoch:7, loss: 0.3493546545529548, weight: [ 0.90553939 -1.57739594]\n",
            "epoch:8, loss: 0.34933351529605366, weight: [ 0.90562074 -1.57753933]\n",
            "epoch:9, loss: 0.3493123782215742, weight: [ 0.9057021  -1.57768272]\n",
            "epoch:10, loss: 0.34929124332920763, weight: [ 0.90578345 -1.5778261 ]\n",
            "epoch:11, loss: 0.3492701106186457, weight: [ 0.90586479 -1.57796948]\n",
            "epoch:12, loss: 0.34924898008957966, weight: [ 0.90594614 -1.57811285]\n",
            "epoch:13, loss: 0.34922785174170146, weight: [ 0.90602748 -1.57825622]\n",
            "epoch:14, loss: 0.3492067255747028, weight: [ 0.90610882 -1.57839958]\n",
            "epoch:15, loss: 0.34918560158827516, weight: [ 0.90619016 -1.57854294]\n",
            "epoch:16, loss: 0.3491644797821107, weight: [ 0.90627149 -1.5786863 ]\n",
            "epoch:17, loss: 0.34914336015590075, weight: [ 0.90635282 -1.57882965]\n",
            "epoch:18, loss: 0.3491222427093378, weight: [ 0.90643415 -1.57897299]\n",
            "epoch:19, loss: 0.34910112744211347, weight: [ 0.90651548 -1.57911633]\n",
            "epoch:20, loss: 0.34908001435391994, weight: [ 0.9065968  -1.57925967]\n",
            "epoch:21, loss: 0.3490589034444492, weight: [ 0.90667812 -1.579403  ]\n",
            "epoch:22, loss: 0.3490377947133934, weight: [ 0.90675944 -1.57954632]\n",
            "epoch:23, loss: 0.3490166881604448, weight: [ 0.90684076 -1.57968964]\n",
            "epoch:24, loss: 0.3489955837852956, weight: [ 0.90692207 -1.57983296]\n",
            "epoch:25, loss: 0.34897448158763816, weight: [ 0.90700338 -1.57997627]\n",
            "epoch:26, loss: 0.3489533815671647, weight: [ 0.90708469 -1.58011958]\n",
            "epoch:27, loss: 0.3489322837235677, weight: [ 0.90716599 -1.58026288]\n",
            "epoch:28, loss: 0.34891118805654, weight: [ 0.9072473  -1.58040618]\n",
            "epoch:29, loss: 0.3488900945657734, weight: [ 0.9073286  -1.58054947]\n",
            "epoch:30, loss: 0.3488690032509611, weight: [ 0.90740989 -1.58069276]\n",
            "epoch:31, loss: 0.34884791411179533, weight: [ 0.90749119 -1.58083604]\n",
            "epoch:32, loss: 0.3488268271479691, weight: [ 0.90757248 -1.58097932]\n",
            "epoch:33, loss: 0.348805742359175, weight: [ 0.90765377 -1.5811226 ]\n",
            "epoch:34, loss: 0.348784659745106, weight: [ 0.90773506 -1.58126587]\n",
            "epoch:35, loss: 0.34876357930545465, weight: [ 0.90781634 -1.58140913]\n",
            "epoch:36, loss: 0.3487425010399142, weight: [ 0.90789762 -1.58155239]\n",
            "epoch:37, loss: 0.3487214249481774, weight: [ 0.9079789  -1.58169565]\n",
            "epoch:38, loss: 0.3487003510299375, weight: [ 0.90806018 -1.5818389 ]\n",
            "epoch:39, loss: 0.34867927928488746, weight: [ 0.90814145 -1.58198214]\n",
            "epoch:40, loss: 0.34865820971272027, weight: [ 0.90822272 -1.58212539]\n",
            "epoch:41, loss: 0.3486371423131293, weight: [ 0.90830399 -1.58226862]\n",
            "epoch:42, loss: 0.34861607708580794, weight: [ 0.90838526 -1.58241185]\n",
            "epoch:43, loss: 0.34859501403044924, weight: [ 0.90846652 -1.58255508]\n",
            "epoch:44, loss: 0.34857395314674666, weight: [ 0.90854778 -1.5826983 ]\n",
            "epoch:45, loss: 0.3485528944343937, weight: [ 0.90862904 -1.58284152]\n",
            "epoch:46, loss: 0.34853183789308373, weight: [ 0.90871029 -1.58298474]\n",
            "epoch:47, loss: 0.34851078352251036, weight: [ 0.90879155 -1.58312794]\n",
            "epoch:48, loss: 0.34848973132236716, weight: [ 0.9088728  -1.58327115]\n",
            "epoch:49, loss: 0.34846868129234776, weight: [ 0.90895404 -1.58341435]\n",
            "epoch:50, loss: 0.34844763343214596, weight: [ 0.90903529 -1.58355754]\n",
            "epoch:51, loss: 0.3484265877414552, weight: [ 0.90911653 -1.58370073]\n",
            "epoch:52, loss: 0.3484055442199698, weight: [ 0.90919777 -1.58384392]\n",
            "epoch:53, loss: 0.34838450286738315, weight: [ 0.90927901 -1.5839871 ]\n",
            "epoch:54, loss: 0.3483634636833894, weight: [ 0.90936024 -1.58413027]\n",
            "epoch:55, loss: 0.3483424266676826, weight: [ 0.90944147 -1.58427344]\n",
            "epoch:56, loss: 0.3483213918199565, weight: [ 0.9095227  -1.58441661]\n",
            "epoch:57, loss: 0.3483003591399057, weight: [ 0.90960393 -1.58455977]\n",
            "epoch:58, loss: 0.3482793286272238, weight: [ 0.90968515 -1.58470293]\n",
            "epoch:59, loss: 0.34825830028160526, weight: [ 0.90976637 -1.58484608]\n",
            "epoch:60, loss: 0.3482372741027443, weight: [ 0.90984759 -1.58498923]\n",
            "epoch:61, loss: 0.3482162500903354, weight: [ 0.90992881 -1.58513237]\n",
            "epoch:62, loss: 0.3481952282440725, weight: [ 0.91001002 -1.58527551]\n",
            "epoch:63, loss: 0.34817420856365044, weight: [ 0.91009123 -1.58541864]\n",
            "epoch:64, loss: 0.3481531910487636, weight: [ 0.91017244 -1.58556177]\n",
            "epoch:65, loss: 0.3481321756991065, weight: [ 0.91025365 -1.5857049 ]\n",
            "epoch:66, loss: 0.34811116251437385, weight: [ 0.91033485 -1.58584802]\n",
            "epoch:67, loss: 0.3480901514942598, weight: [ 0.91041605 -1.58599113]\n",
            "epoch:68, loss: 0.3480691426384597, weight: [ 0.91049725 -1.58613424]\n",
            "epoch:69, loss: 0.34804813594666795, weight: [ 0.91057844 -1.58627735]\n",
            "epoch:70, loss: 0.3480271314185795, weight: [ 0.91065963 -1.58642045]\n",
            "epoch:71, loss: 0.34800612905388895, weight: [ 0.91074082 -1.58656355]\n",
            "epoch:72, loss: 0.3479851288522917, weight: [ 0.91082201 -1.58670664]\n",
            "epoch:73, loss: 0.3479641308134823, weight: [ 0.9109032  -1.58684972]\n",
            "epoch:74, loss: 0.34794313493715606, weight: [ 0.91098438 -1.58699281]\n",
            "epoch:75, loss: 0.34792214122300796, weight: [ 0.91106556 -1.58713588]\n",
            "epoch:76, loss: 0.3479011496707332, weight: [ 0.91114673 -1.58727896]\n",
            "epoch:77, loss: 0.3478801602800267, weight: [ 0.91122791 -1.58742202]\n",
            "epoch:78, loss: 0.3478591730505844, weight: [ 0.91130908 -1.58756509]\n",
            "epoch:79, loss: 0.34783818798210087, weight: [ 0.91139025 -1.58770815]\n",
            "epoch:80, loss: 0.34781720507427183, weight: [ 0.91147142 -1.5878512 ]\n",
            "epoch:81, loss: 0.34779622432679264, weight: [ 0.91155258 -1.58799425]\n",
            "epoch:82, loss: 0.347775245739359, weight: [ 0.91163374 -1.5881373 ]\n",
            "epoch:83, loss: 0.3477542693116661, weight: [ 0.9117149  -1.58828034]\n",
            "epoch:84, loss: 0.34773329504340966, weight: [ 0.91179605 -1.58842337]\n",
            "epoch:85, loss: 0.34771232293428544, weight: [ 0.91187721 -1.5885664 ]\n",
            "epoch:86, loss: 0.34769135298398907, weight: [ 0.91195836 -1.58870943]\n",
            "epoch:87, loss: 0.34767038519221627, weight: [ 0.91203951 -1.58885245]\n",
            "epoch:88, loss: 0.34764941955866285, weight: [ 0.91212065 -1.58899547]\n",
            "epoch:89, loss: 0.34762845608302484, weight: [ 0.91220179 -1.58913848]\n",
            "epoch:90, loss: 0.3476074947649979, weight: [ 0.91228293 -1.58928149]\n",
            "epoch:91, loss: 0.3475865356042782, weight: [ 0.91236407 -1.58942449]\n",
            "epoch:92, loss: 0.34756557860056175, weight: [ 0.91244521 -1.58956749]\n",
            "epoch:93, loss: 0.34754462375354467, weight: [ 0.91252634 -1.58971048]\n",
            "epoch:94, loss: 0.347523671062923, weight: [ 0.91260747 -1.58985347]\n",
            "epoch:95, loss: 0.347502720528393, weight: [ 0.9126886  -1.58999645]\n",
            "epoch:96, loss: 0.3474817721496508, weight: [ 0.91276972 -1.59013943]\n",
            "epoch:97, loss: 0.3474608259263928, weight: [ 0.91285084 -1.59028241]\n",
            "epoch:98, loss: 0.34743988185831576, weight: [ 0.91293196 -1.59042538]\n",
            "epoch:99, loss: 0.3474189399451156, weight: [ 0.91301308 -1.59056834]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict\n",
        "y_preds = []\n",
        "for y_pred in sigmoid(w, x_test):\n",
        "  if y_pred > 0.5:\n",
        "    y_pred = 1\n",
        "  else:\n",
        "    y_pred = 0\n",
        "  y_preds.append(y_pred)\n",
        "\n",
        "y_preds = np.array(y_preds)\n",
        "y_preds, y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY2yc-O2aO3K",
        "outputId": "2334cd2d-e897-4af2-db3f-75b874bf2137"
      },
      "execution_count": 404,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0]),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 404
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "for x1, x2 in x_train:\n",
        "  plt.scatter(sigmoid(w, x1), sigmoid(w, x2), color='blue')\n",
        "plt.plot(sigmoid(w, x_train))\n",
        "plt.xlim([-0.1, 1.1])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "9HmqxqOhm7DR",
        "outputId": "fb2a71aa-9ffb-40a3-dc69-a43b92f763a8"
      },
      "execution_count": 403,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch logistic regression"
      ],
      "metadata": {
        "id": "6mT4nORKnKvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "iris = datasets.load_iris()\n",
        "x_train = torch.tensor(iris.data[:-50, :2], dtype=torch.float32)  # we only take the first two features.\n",
        "y_train = torch.tensor(iris.target[:-50], dtype=torch.float32)"
      ],
      "metadata": {
        "id": "4qVyBSiao0Lt"
      },
      "execution_count": 394,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression(torch.nn.Module):\n",
        "  def __init__(self, inputDim, outputDim):\n",
        "    super(LogisticRegression, self).__init__()\n",
        "    self.linear = torch.nn.Linear(inputDim, outputDim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = torch.sigmoid(self.linear(x))\n",
        "    return output"
      ],
      "metadata": {
        "id": "-SbAcEQZOTuM"
      },
      "execution_count": 395,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(2, 1)\n",
        "lr = 0.1\n",
        "loss = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "epoch_num = 100\n",
        "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "GndhHX1fnKNF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ff7fee1-88e7-42ff-f156-f30ead6bf286"
      },
      "execution_count": 396,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-396-30ccdd14d0ef>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x_train = torch.tensor(x_train, dtype=torch.float32)\n",
            "<ipython-input-396-30ccdd14d0ef>:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_train = torch.tensor(y_train, dtype=torch.float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epoch_num):\n",
        "  optimizer.zero_grad()\n",
        "  output = model(x_train)\n",
        "  l = loss(torch.squeeze(output), y_train)\n",
        "  l.backward()\n",
        "  optimizer.step()\n",
        "  print(f'epoch:{epoch+1}, loss:{l}, weigts:{[(w, b) for w, b in model.named_parameters()]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkujT9ATn_Re",
        "outputId": "a0929c4a-0275-4cb0-c033-70d33086d208"
      },
      "execution_count": 397,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1, loss:0.9292832016944885, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.5101, -0.6649]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1054], requires_grad=True))]\n",
            "epoch:2, loss:0.5409089922904968, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.4497, -0.7215]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1197], requires_grad=True))]\n",
            "epoch:3, loss:0.5008278489112854, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.4541, -0.7409]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1221], requires_grad=True))]\n",
            "epoch:4, loss:0.49700504541397095, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.4634, -0.7574]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1237], requires_grad=True))]\n",
            "epoch:5, loss:0.4934260845184326, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.4728, -0.7736]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1252], requires_grad=True))]\n",
            "epoch:6, loss:0.48989954590797424, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.4821, -0.7897]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1266], requires_grad=True))]\n",
            "epoch:7, loss:0.4864242672920227, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.4914, -0.8057]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1281], requires_grad=True))]\n",
            "epoch:8, loss:0.48299914598464966, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.5007, -0.8215]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1296], requires_grad=True))]\n",
            "epoch:9, loss:0.4796232581138611, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.5098, -0.8373]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1310], requires_grad=True))]\n",
            "epoch:10, loss:0.4762958288192749, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.5189, -0.8529]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1325], requires_grad=True))]\n",
            "epoch:11, loss:0.47301578521728516, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.5280, -0.8684]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1339], requires_grad=True))]\n",
            "epoch:12, loss:0.469782292842865, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.5370, -0.8838]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1353], requires_grad=True))]\n",
            "epoch:13, loss:0.46659454703330994, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.5459, -0.8991]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1368], requires_grad=True))]\n",
            "epoch:14, loss:0.4634515643119812, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.5547, -0.9143]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1382], requires_grad=True))]\n",
            "epoch:15, loss:0.46035265922546387, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.5635, -0.9294]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1396], requires_grad=True))]\n",
            "epoch:16, loss:0.4572969377040863, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.5722, -0.9444]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1410], requires_grad=True))]\n",
            "epoch:17, loss:0.45428362488746643, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.5809, -0.9592]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1424], requires_grad=True))]\n",
            "epoch:18, loss:0.45131194591522217, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.5895, -0.9740]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1438], requires_grad=True))]\n",
            "epoch:19, loss:0.4483811557292938, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.5981, -0.9886]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1452], requires_grad=True))]\n",
            "epoch:20, loss:0.44549044966697693, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.6066, -1.0032]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1465], requires_grad=True))]\n",
            "epoch:21, loss:0.4426390826702118, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.6150, -1.0177]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1479], requires_grad=True))]\n",
            "epoch:22, loss:0.43982645869255066, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.6234, -1.0320]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1493], requires_grad=True))]\n",
            "epoch:23, loss:0.43705177307128906, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.6317, -1.0463]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1506], requires_grad=True))]\n",
            "epoch:24, loss:0.4343143105506897, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.6400, -1.0605]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1520], requires_grad=True))]\n",
            "epoch:25, loss:0.4316134750843048, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.6482, -1.0746]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1533], requires_grad=True))]\n",
            "epoch:26, loss:0.4289485216140747, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.6563, -1.0885]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1547], requires_grad=True))]\n",
            "epoch:27, loss:0.426318883895874, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.6644, -1.1024]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1560], requires_grad=True))]\n",
            "epoch:28, loss:0.42372381687164307, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.6725, -1.1162]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1573], requires_grad=True))]\n",
            "epoch:29, loss:0.421162873506546, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.6805, -1.1299]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1586], requires_grad=True))]\n",
            "epoch:30, loss:0.4186353385448456, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.6885, -1.1435]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1599], requires_grad=True))]\n",
            "epoch:31, loss:0.4161405861377716, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.6964, -1.1571]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1613], requires_grad=True))]\n",
            "epoch:32, loss:0.4136780798435211, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.7042, -1.1705]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1626], requires_grad=True))]\n",
            "epoch:33, loss:0.4112473428249359, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.7120, -1.1838]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1639], requires_grad=True))]\n",
            "epoch:34, loss:0.4088475704193115, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.7198, -1.1971]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1651], requires_grad=True))]\n",
            "epoch:35, loss:0.4064784348011017, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.7275, -1.2103]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1664], requires_grad=True))]\n",
            "epoch:36, loss:0.40413936972618103, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.7351, -1.2234]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1677], requires_grad=True))]\n",
            "epoch:37, loss:0.40182968974113464, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.7427, -1.2364]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1690], requires_grad=True))]\n",
            "epoch:38, loss:0.39954906702041626, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.7503, -1.2493]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1703], requires_grad=True))]\n",
            "epoch:39, loss:0.3972969949245453, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.7578, -1.2622]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1715], requires_grad=True))]\n",
            "epoch:40, loss:0.3950727880001068, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.7653, -1.2749]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1728], requires_grad=True))]\n",
            "epoch:41, loss:0.39287617802619934, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.7727, -1.2876]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1740], requires_grad=True))]\n",
            "epoch:42, loss:0.3907066285610199, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.7801, -1.3002]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1753], requires_grad=True))]\n",
            "epoch:43, loss:0.3885635733604431, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.7874, -1.3128]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1765], requires_grad=True))]\n",
            "epoch:44, loss:0.38644665479660034, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.7947, -1.3252]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1778], requires_grad=True))]\n",
            "epoch:45, loss:0.38435545563697815, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.8019, -1.3376]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1790], requires_grad=True))]\n",
            "epoch:46, loss:0.38228949904441833, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.8091, -1.3499]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1802], requires_grad=True))]\n",
            "epoch:47, loss:0.38024836778640747, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.8163, -1.3621]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1814], requires_grad=True))]\n",
            "epoch:48, loss:0.37823161482810974, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.8234, -1.3743]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1826], requires_grad=True))]\n",
            "epoch:49, loss:0.3762388527393341, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.8305, -1.3864]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1839], requires_grad=True))]\n",
            "epoch:50, loss:0.37426963448524475, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.8375, -1.3984]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1851], requires_grad=True))]\n",
            "epoch:51, loss:0.3723236918449402, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.8445, -1.4103]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1863], requires_grad=True))]\n",
            "epoch:52, loss:0.3704005181789398, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.8514, -1.4222]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1875], requires_grad=True))]\n",
            "epoch:53, loss:0.3684997260570526, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.8583, -1.4340]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1887], requires_grad=True))]\n",
            "epoch:54, loss:0.36662110686302185, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.8652, -1.4457]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1898], requires_grad=True))]\n",
            "epoch:55, loss:0.36476412415504456, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.8720, -1.4574]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1910], requires_grad=True))]\n",
            "epoch:56, loss:0.36292847990989685, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.8788, -1.4690]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1922], requires_grad=True))]\n",
            "epoch:57, loss:0.36111390590667725, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.8856, -1.4805]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1934], requires_grad=True))]\n",
            "epoch:58, loss:0.35931992530822754, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.8923, -1.4920]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1946], requires_grad=True))]\n",
            "epoch:59, loss:0.35754629969596863, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.8990, -1.5034]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1957], requires_grad=True))]\n",
            "epoch:60, loss:0.3557926118373871, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.9056, -1.5147]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1969], requires_grad=True))]\n",
            "epoch:61, loss:0.35405871272087097, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.9122, -1.5260]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1980], requires_grad=True))]\n",
            "epoch:62, loss:0.3523441255092621, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.9188, -1.5372]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.1992], requires_grad=True))]\n",
            "epoch:63, loss:0.35064858198165894, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.9253, -1.5483]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2003], requires_grad=True))]\n",
            "epoch:64, loss:0.3489718735218048, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.9318, -1.5594]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2015], requires_grad=True))]\n",
            "epoch:65, loss:0.3473135232925415, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.9383, -1.5704]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2026], requires_grad=True))]\n",
            "epoch:66, loss:0.34567341208457947, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.9447, -1.5814]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2038], requires_grad=True))]\n",
            "epoch:67, loss:0.34405118227005005, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.9511, -1.5923]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2049], requires_grad=True))]\n",
            "epoch:68, loss:0.342446506023407, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.9575, -1.6031]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2060], requires_grad=True))]\n",
            "epoch:69, loss:0.34085914492607117, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.9638, -1.6139]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2071], requires_grad=True))]\n",
            "epoch:70, loss:0.33928894996643066, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.9701, -1.6246]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2083], requires_grad=True))]\n",
            "epoch:71, loss:0.33773547410964966, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.9764, -1.6353]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2094], requires_grad=True))]\n",
            "epoch:72, loss:0.3361985385417938, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.9826, -1.6459]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2105], requires_grad=True))]\n",
            "epoch:73, loss:0.33467796444892883, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.9888, -1.6564]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2116], requires_grad=True))]\n",
            "epoch:74, loss:0.33317333459854126, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 0.9949, -1.6669]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2127], requires_grad=True))]\n",
            "epoch:75, loss:0.33168452978134155, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.0011, -1.6774]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2138], requires_grad=True))]\n",
            "epoch:76, loss:0.330211341381073, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.0072, -1.6877]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2149], requires_grad=True))]\n",
            "epoch:77, loss:0.32875344157218933, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.0132, -1.6981]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2160], requires_grad=True))]\n",
            "epoch:78, loss:0.3273106515407562, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.0193, -1.7083]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2171], requires_grad=True))]\n",
            "epoch:79, loss:0.3258826732635498, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.0253, -1.7186]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2181], requires_grad=True))]\n",
            "epoch:80, loss:0.3244693875312805, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.0312, -1.7287]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2192], requires_grad=True))]\n",
            "epoch:81, loss:0.32307061553001404, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.0372, -1.7389]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2203], requires_grad=True))]\n",
            "epoch:82, loss:0.3216859698295593, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.0431, -1.7489]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2214], requires_grad=True))]\n",
            "epoch:83, loss:0.32031524181365967, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.0490, -1.7589]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2224], requires_grad=True))]\n",
            "epoch:84, loss:0.31895849108695984, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.0548, -1.7689]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2235], requires_grad=True))]\n",
            "epoch:85, loss:0.31761521100997925, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.0606, -1.7788]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2246], requires_grad=True))]\n",
            "epoch:86, loss:0.3162854015827179, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.0664, -1.7887]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2256], requires_grad=True))]\n",
            "epoch:87, loss:0.3149687647819519, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.0722, -1.7985]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2267], requires_grad=True))]\n",
            "epoch:88, loss:0.3136652112007141, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.0780, -1.8082]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2277], requires_grad=True))]\n",
            "epoch:89, loss:0.31237441301345825, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.0837, -1.8180]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2288], requires_grad=True))]\n",
            "epoch:90, loss:0.31109628081321716, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.0893, -1.8276]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2298], requires_grad=True))]\n",
            "epoch:91, loss:0.3098306357860565, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.0950, -1.8373]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2309], requires_grad=True))]\n",
            "epoch:92, loss:0.3085772693157196, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.1006, -1.8468]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2319], requires_grad=True))]\n",
            "epoch:93, loss:0.3073360323905945, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.1062, -1.8564]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2330], requires_grad=True))]\n",
            "epoch:94, loss:0.3061067461967468, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.1118, -1.8658]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2340], requires_grad=True))]\n",
            "epoch:95, loss:0.3048892021179199, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.1174, -1.8753]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2350], requires_grad=True))]\n",
            "epoch:96, loss:0.3036832809448242, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.1229, -1.8847]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2360], requires_grad=True))]\n",
            "epoch:97, loss:0.302488774061203, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.1284, -1.8940]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2371], requires_grad=True))]\n",
            "epoch:98, loss:0.3013056516647339, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.1339, -1.9033]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2381], requires_grad=True))]\n",
            "epoch:99, loss:0.300133615732193, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.1393, -1.9126]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2391], requires_grad=True))]\n",
            "epoch:100, loss:0.29897257685661316, weigts:[('linear.weight', Parameter containing:\n",
            "tensor([[ 1.1447, -1.9218]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
            "tensor([-0.2401], requires_grad=True))]\n"
          ]
        }
      ]
    }
  ]
}